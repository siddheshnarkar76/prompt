import asyncio
import glob
import json
import os
from datetime import datetime
from typing import Any, Dict, List

import httpx
import pdfplumber
from prefect import flow, get_run_logger, task
from prefect.cache_policies import INPUTS
from prefect.tasks import task_input_hash


@task(cache_policy=INPUTS)
def ingest_pdf(file_path: str) -> Dict[str, Any]:
    """Extract comprehensive data from PDF including text, tables, and metadata"""
    logger = get_run_logger()

    try:
        data = {
            "file_path": file_path,
            "timestamp": datetime.now().isoformat(),
            "text": "",
            "tables": [],
            "metadata": {},
            "page_count": 0,
        }

        with pdfplumber.open(file_path) as pdf:
            data["page_count"] = len(pdf.pages)
            data["metadata"] = pdf.metadata or {}

            # Extract text from all pages
            text_pages = []
            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text() or ""
                text_pages.append(f"--- Page {i+1} ---\n{page_text}")

                # Extract tables if present
                tables = page.extract_tables()
                if tables:
                    data["tables"].extend([{"page": i + 1, "table": table} for table in tables])

            data["text"] = "\n\n".join(text_pages)

        logger.info(f"Successfully ingested PDF: {file_path} ({data['page_count']} pages)")
        return data

    except Exception as e:
        logger.error(f"Failed to ingest PDF {file_path}: {e}")
        raise


@task
async def apply_mcp_rules(pdf_data: Dict[str, Any], city: str = "Mumbai") -> Dict[str, Any]:
    """Apply MCP compliance rules to extracted PDF data"""
    logger = get_run_logger()

    try:
        # Prepare MCP request
        mcp_payload = {
            "city": city,
            "document_text": pdf_data.get("text", ""),
            "tables": pdf_data.get("tables", []),
            "metadata": pdf_data.get("metadata", {}),
            "source_file": pdf_data.get("file_path", "unknown"),
        }

        # Call Sohum's MCP API
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    "https://ai-rule-api-w7z5.onrender.com/mcp/document/analyze", json=mcp_payload
                )
                response.raise_for_status()
                result = response.json()

                logger.info(f"MCP rules applied successfully for {city}")
                return result

            except Exception as e:
                logger.warning(f"External MCP failed: {e}, using fallback")

                # Fallback compliance analysis
                compliance_result = {
                    "case_id": f"fallback_{city}_{hash(pdf_data.get('file_path', '')) % 10000}",
                    "city": city,
                    "compliant": True,
                    "confidence_score": 0.75,
                    "violations": [],
                    "recommendations": ["Manual review recommended", "Verify local building codes"],
                    "rules_applied": ["FALLBACK-BASIC-CHECK"],
                    "processing_time_ms": 100,
                }

                return compliance_result

    except Exception as e:
        logger.error(f"MCP rules application failed: {e}")
        raise


@task
def save_json(data: dict, output_path: str):
    """Save the given data dict as JSON to the specified path."""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(data, f, indent=2)
    logger = get_run_logger()
    logger.info(f"Saved output JSON to {output_path}")


@flow(name="MCP_Compliance_Workflow")
async def mcp_compliance_flow(
    input_dir: str = "data/pdfs/incoming", output_dir: str = "data/compliance_output", city: str = "Mumbai"
):
    """
    Complete Prefect flow that replaces n8n PDF ingestion â†’ MCP workflow.
    Scans for PDFs, processes each with MCP rules, and saves JSON outputs.
    """
    logger = get_run_logger()
    logger.info(f"Starting MCP compliance flow for {city}")

    # Ensure directories exist
    os.makedirs(input_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)

    pdf_paths = glob.glob(os.path.join(input_dir, "*.pdf"))
    logger.info(f"Found {len(pdf_paths)} PDF files to process")

    processed_files = []

    for pdf_path in pdf_paths:
        try:
            # Extract PDF data
            data = ingest_pdf(pdf_path)

            # Apply MCP compliance rules
            result = await apply_mcp_rules(data, city)

            # Save compliance result
            base = os.path.splitext(os.path.basename(pdf_path))[0]
            output_file = f"{output_dir}/{base}_compliance_{city.lower()}.json"
            save_json(result, output_file)

            processed_files.append(
                {
                    "input_file": pdf_path,
                    "output_file": output_file,
                    "case_id": result.get("case_id"),
                    "compliant": result.get("compliant", False),
                }
            )

            logger.info(f"Processed {pdf_path} -> {output_file}")

        except Exception as e:
            logger.error(f"Failed to process {pdf_path}: {e}")

    logger.info(f"MCP compliance flow completed: {len(processed_files)} files processed")
    return {"processed_files": processed_files, "city": city, "total_processed": len(processed_files)}


@task
def aggregate_logs(log_dir: str) -> dict:
    """Aggregate logs from multiple runs into a single summary."""
    combined = {"entries": []}
    for log_file in glob.glob(f"{log_dir}/*.json"):
        with open(log_file) as f:
            log = json.load(f)
            combined["entries"].append(log)
    return combined


@task
def verify_geometry(output_glb: str) -> bool:
    """Verify a .glb geometry file (placeholder logic)."""
    exists = os.path.exists(output_glb)
    # Here one could add actual GLB validation, 3D checks, etc.
    return exists


@flow(name="Log_Aggregation_Workflow")
def log_aggregation_flow(log_dir: str = "data/logs", summary_path: str = "data/logs/summary.json"):
    summary = aggregate_logs(log_dir)
    save_json(summary, summary_path)


@flow(name="Geometry_Verification_Workflow")
def geometry_verification_flow(geometry_dir: str = "data/geometry"):
    all_valid = True
    for glb_file in glob.glob(f"{geometry_dir}/*.glb"):
        valid = verify_geometry(glb_file)
        if not valid:
            get_run_logger().warning(f"Geometry file failed verification: {glb_file}")
            all_valid = False
    return all_valid
